{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (3.4.5)\r\n",
      "Requirement already satisfied: six in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (from nltk) (1.14.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posix\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shijia_huang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shijia_huang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shijia_huang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shijia_huang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/shijia_huang/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/shijia_huang/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import twitter_samples \n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "\n",
    "import re, string, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>sent_text</th>\n",
       "      <th>sentiment_human</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2151</td>\n",
       "      <td>0</td>\n",
       "      <td>I love the MDX, It blows away the Lexus for th...</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2151</td>\n",
       "      <td>1</td>\n",
       "      <td>but doesn't have any of the features that are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2151</td>\n",
       "      <td>2</td>\n",
       "      <td>The Nav system &amp; the entertainment system are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2151</td>\n",
       "      <td>3</td>\n",
       "      <td>It's the best luxury vehicle for the money.</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2162</td>\n",
       "      <td>0</td>\n",
       "      <td>i love my mdx</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2980</td>\n",
       "      <td>0</td>\n",
       "      <td>After a year and two road trips, here are my g...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2980</td>\n",
       "      <td>1</td>\n",
       "      <td>Pros:Great mileage (28 mpg on the road), comfo...</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2980</td>\n",
       "      <td>2</td>\n",
       "      <td>Cons:Interior finish uninspiring, fit and fini...</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2980</td>\n",
       "      <td>3</td>\n",
       "      <td>We've had the car in for two fit and finish re...</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id  sent_idx                                          sent_text  \\\n",
       "0       2151         0  I love the MDX, It blows away the Lexus for th...   \n",
       "1       2151         1  but doesn't have any of the features that are ...   \n",
       "2       2151         2  The Nav system & the entertainment system are ...   \n",
       "3       2151         3       It's the best luxury vehicle for the money.    \n",
       "4       2162         0                                      i love my mdx   \n",
       "5       2980         0  After a year and two road trips, here are my g...   \n",
       "6       2980         1  Pros:Great mileage (28 mpg on the road), comfo...   \n",
       "7       2980         2  Cons:Interior finish uninspiring, fit and fini...   \n",
       "8       2980         3  We've had the car in for two fit and finish re...   \n",
       "\n",
       "   sentiment_human  length  \n",
       "0                1      93  \n",
       "1                0      69  \n",
       "2                1      87  \n",
       "3                1      44  \n",
       "4                1      13  \n",
       "5                1      65  \n",
       "6                1     140  \n",
       "7                0     195  \n",
       "8                0      52  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.dropbox.com/s/wt3dyh2thhkhmxg/qualtric_sentiment.csv?dl=1\"\n",
    "df = pd.read_csv(url)\n",
    "df.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206\n",
      "783\n",
      "we are cool\n"
     ]
    }
   ],
   "source": [
    "# df.loc[df['length']>df['length'].mean(), 'is_short']=0\n",
    " \n",
    "\n",
    "test_text_pos = []\n",
    "test_text_neg = []\n",
    "for index in range(int(len(df.index))):\n",
    "    if df.at[index,'sentiment_human'] == 1:\n",
    "        test_text_pos.append(df.at[index,'sent_text'])\n",
    "    if df.at[index,'sentiment_human'] == 0:\n",
    "        test_text_neg.append(df.at[index,'sent_text'])\n",
    "\n",
    "\n",
    "print(len(test_text_pos))\n",
    "print(len(test_text_neg))\n",
    "\n",
    "if(test_text_neg[2] != df.at[8,'sent_text']):\n",
    "    print(\"something went wrong\")\n",
    "else:\n",
    "    print(\"we are cool\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_folder(path):\n",
    "    file_names = []\n",
    "    text_list = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"): \n",
    "            file_names.append(path + filename)\n",
    "            #print(filename)  \n",
    "    for file in file_names:\n",
    "        f = open(file,\"r+\")\n",
    "        content = f.read()\n",
    "        text_list.append(content)\n",
    "        f.close()\n",
    "    return text_list, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_neg, train_file_neg = read_from_folder(\"/Users/shijia_huang/Desktop/The Thing/aclImdb/train/neg/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_pos, train_file_pos = read_from_folder(\"/Users/shijia_huang/Desktop/The Thing/aclImdb/train/pos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_text_unsup, train_file_unsup = read_from_folder(\"/Users/shijia_huang/Desktop/The Thing/aclImdb/train/unsup/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_token_list):\n",
    "    for tweet_tokens in cleaned_token_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_tweets = train_text_pos\n",
    "negative_train_tweets = train_text_neg\n",
    "#unsup_train_tweets = train_text_unsup \n",
    "\n",
    "positive_train_tweet_tokens = [word_tokenize(i) for i in positive_train_tweets]\n",
    "negative_train_tweet_tokens = [word_tokenize(i) for i in negative_train_tweets]\n",
    "#unsup_train_tweet_tokens = [word_tokenize(i) for i in unsup_train_tweets]\n",
    "\n",
    "positive_train_cleaned_tokens_list = []\n",
    "negative_train_cleaned_tokens_list = []\n",
    "#unsup_train_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_train_tweet_tokens:\n",
    "    positive_train_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_train_tweet_tokens:\n",
    "    negative_train_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "    \n",
    "# for tokens in unsup_train_tweet_tokens:\n",
    "#     unsup_train_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "#print(positive_cleaned_tokens_list)\n",
    "# all_pos_words = get_all_words(positive_train_cleaned_tokens_list)\n",
    "# #print(all_pos_words)\n",
    "# freq_dist_pos = FreqDist(all_pos_words)\n",
    "# print(freq_dist_pos.most_common(10))\n",
    "\n",
    "\n",
    "\n",
    "positive_train_tokens_for_model = get_tweets_for_model(positive_train_cleaned_tokens_list)\n",
    "negative_train_tokens_for_model = get_tweets_for_model(negative_train_cleaned_tokens_list)\n",
    "# unsup_train_tokens_for_model = get_tweets_for_model(unsup_train_cleaned_tokens_list)\n",
    "\n",
    "positive_train_dataset = [ (tweet_dict, \"Positive\") \n",
    "                      for tweet_dict in positive_train_tokens_for_model]\n",
    "negative_train_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_train_tokens_for_model]\n",
    "# unsup_train_dataset = [(tweet_dict, \"Unsup\")\n",
    "#                      for tweet_dict in unsup_train_tokens_for_model]\n",
    "\n",
    "dataset = positive_train_dataset + negative_train_dataset \n",
    "\n",
    "train_data = dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_test_tweets = test_text_pos\n",
    "negative_test_tweets = test_text_neg\n",
    "# text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "#tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "\n",
    "positive_test_tweet_tokens = [word_tokenize(i) for i in positive_test_tweets]\n",
    "negative_test_tweet_tokens = [word_tokenize(i) for i in negative_test_tweets]\n",
    "\n",
    "positive_test_cleaned_tokens_list = []\n",
    "negative_test_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_test_tweet_tokens:\n",
    "    positive_test_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_test_tweet_tokens:\n",
    "    negative_test_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "positive_test_tokens_for_model = get_tweets_for_model(positive_test_cleaned_tokens_list)\n",
    "negative_test_tokens_for_model = get_tweets_for_model(negative_test_cleaned_tokens_list)\n",
    "\n",
    "positive_test_dataset = [ (tweet_dict, \"Positive\") \n",
    "                      for tweet_dict in positive_test_tokens_for_model]\n",
    "negative_test_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_test_tokens_for_model]\n",
    "\n",
    "dataset = positive_test_dataset + negative_test_dataset\n",
    "\n",
    "test_data = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.614586818333891\n",
      "Most Informative Features\n",
      "                    2/10 = True           Negati : Positi =     76.3 : 1.0\n",
      "                    4/10 = True           Negati : Positi =     64.5 : 1.0\n",
      "                    *1/2 = True           Negati : Positi =     57.0 : 1.0\n",
      "                    3/10 = True           Negati : Positi =     44.1 : 1.0\n",
      "                    boll = True           Negati : Positi =     39.0 : 1.0\n",
      "                     uwe = True           Negati : Positi =     37.6 : 1.0\n",
      "                    7/10 = True           Positi : Negati =     33.2 : 1.0\n",
      "                    8/10 = True           Positi : Negati =     27.6 : 1.0\n",
      "             unwatchable = True           Negati : Positi =     27.0 : 1.0\n",
      "                  awful. = True           Negati : Positi =     27.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
